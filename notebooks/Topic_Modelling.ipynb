{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gensim\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim import corpora\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import string\n",
    "import os\n",
    "import re\n",
    "import  sys\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add parent directory to path to import modules from src\n",
    "rpath = os.path.abspath('C:/Users/moyka/OneDrive/Documents/GitHub/week0_starter_network_analysis')\n",
    "if rpath not in sys.path:\n",
    "    sys.path.insert(0, rpath)\n",
    "\n",
    "from src.loader import SlackDataLoader\n",
    "import src.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_loader = SlackDataLoader(r\"C:/Users/moyka/Desktop/Dashboard/anonymized/\")\n",
    "\n",
    "def get_channel_messages(channel):\n",
    "    channel_messages = utils.get_messages_on_channel(f\"C:/Users/moyka/Desktop/Dashboard/anonymized/{channel}\") \n",
    "    # Create an empty DataFrame\n",
    "    df = pd.DataFrame(channel_messages)\n",
    "    return df\n",
    "\n",
    "def get_all_channels_message():\n",
    "    dfs = []  # List to store individual DataFrames\n",
    "\n",
    "    for channel in data_loader.channels:\n",
    "        dfs.append(get_channel_messages(channel[\"name\"]))\n",
    "\n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    result_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Extract and remove URLs\n",
    "    urls = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text)\n",
    "    for url in urls:\n",
    "        text = text.replace(url, '')\n",
    "\n",
    "    text = re.sub(r'<@.*?>', '', text)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Perform stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "    # Perform lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    # Join the tokens back into a string\n",
    "    text = ' '.join(tokens)\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_data(df):\n",
    "    df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
    "    sentence_list = [tweet for tweet in df['cleaned_text']]\n",
    "    word_list = [sent.split() for sent in sentence_list]\n",
    "\n",
    "    #Create dictionary which contains Id and word\n",
    "    word_to_id = corpora.Dictionary(word_list) #generate unique tokens\n",
    "    corpus = [word_to_id.doc2bow(tweet) for tweet in word_list]\n",
    "    \n",
    "    return df, word_list, word_to_id, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(corpus, word_to_id):\n",
    "    # Build LDA model\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus,\n",
    "                                        id2word=word_to_id,\n",
    "                                        num_topics=5,\n",
    "                                        random_state=100,\n",
    "                                        update_every=1,\n",
    "                                        chunksize=100,\n",
    "                                        passes=10,\n",
    "                                        alpha='auto',\n",
    "                                        per_word_topics=True)    \n",
    "    return lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_topics(lda_model):\n",
    "    pprint(lda_model.show_topics(formatted=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_analysis(lda_model, corpus, word_list, word_to_id):\n",
    "    print('\\nPerplexity: ', lda_model.log_perplexity(corpus))\n",
    "    doc_lda = lda_model[corpus]\n",
    "\n",
    "\n",
    "    # Compute Coherence Score\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=word_list, dictionary=word_to_id, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    print('\\n Lda model Coherence Score/Accuracy on Tweets: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_topics(df):\n",
    "    df, word_list, word_to_id, corpus = prepare_data(df)\n",
    "    lda_model = build_model(corpus, word_to_id)\n",
    "\n",
    "    # Show the top 10 topics\n",
    "    show_topics(lda_model)\n",
    "    \n",
    "\n",
    "    # Visualize the top 10 topics\n",
    "    pyLDAvis.enable_notebook()\n",
    "    LDAvis_prepared = gensimvis.prepare(lda_model, corpus, word_to_id)\n",
    "    return LDAvis_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 topics of the different channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'src.utils' has no attribute 'get_messages_on_channel'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m get_channel_messages(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall-week1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m get_top_topics(df)\n",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m, in \u001b[0;36mget_channel_messages\u001b[1;34m(channel)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_channel_messages\u001b[39m(channel):\n\u001b[1;32m----> 4\u001b[0m     channel_messages \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mget_messages_on_channel(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/moyka/Desktop/Dashboard/anonymized/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchannel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Create an empty DataFrame\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(channel_messages)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'src.utils' has no attribute 'get_messages_on_channel'"
     ]
    }
   ],
   "source": [
    "df = get_channel_messages(\"all-week1\")\n",
    "get_top_topics(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\moyka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'src.utils' has no attribute 'get_messages_on_channel'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m get_channel_messages(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall-community-building\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m get_top_topics(df)\n",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m, in \u001b[0;36mget_channel_messages\u001b[1;34m(channel)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_channel_messages\u001b[39m(channel):\n\u001b[1;32m----> 4\u001b[0m     channel_messages \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mget_messages_on_channel(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/moyka/Desktop/Dashboard/anonymized/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchannel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Create an empty DataFrame\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(channel_messages)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'src.utils' has no attribute 'get_messages_on_channel'"
     ]
    }
   ],
   "source": [
    "df = get_channel_messages(\"all-community-building\")\n",
    "get_top_topics(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 topics of all channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'src.utils' has no attribute 'get_messages_on_channel'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m get_all_channels_message()\n\u001b[0;32m      2\u001b[0m get_top_topics(df)\n",
      "Cell \u001b[1;32mIn[3], line 13\u001b[0m, in \u001b[0;36mget_all_channels_message\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m dfs \u001b[38;5;241m=\u001b[39m []  \u001b[38;5;66;03m# List to store individual DataFrames\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m channel \u001b[38;5;129;01min\u001b[39;00m data_loader\u001b[38;5;241m.\u001b[39mchannels:\n\u001b[1;32m---> 13\u001b[0m     dfs\u001b[38;5;241m.\u001b[39mappend(get_channel_messages(channel[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Concatenate all DataFrames into a single DataFrame\u001b[39;00m\n\u001b[0;32m     16\u001b[0m result_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(dfs, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m, in \u001b[0;36mget_channel_messages\u001b[1;34m(channel)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_channel_messages\u001b[39m(channel):\n\u001b[1;32m----> 4\u001b[0m     channel_messages \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mget_messages_on_channel(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/moyka/Desktop/Dashboard/anonymized/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchannel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Create an empty DataFrame\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(channel_messages)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'src.utils' has no attribute 'get_messages_on_channel'"
     ]
    }
   ],
   "source": [
    "df = get_all_channels_message()\n",
    "get_top_topics(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
